
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Market Basket Analysis with MATLAB</title><meta name="generator" content="MATLAB 8.3"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2014-08-27"><meta name="DC.source" content="marketBasket.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Market Basket Analysis with MATLAB</h1><!--introduction--><p>When you hear "Data Mining", the "beer and diapers" story comes to mind - supermarkets placing beer next to diapers because they mined their POS (Point-of-Sales) data and found that men often bought those two items together.</p><p>This is known as <b>Association Analysis</b> or <b>Frequent Itemset Mining</b>. I don't work at a supermarket but I wanted to play with this technique for web usage pattern mining. Fortunately, a pseudo code for this algorithm is available in Chapter 6 (sample chapter available for free download) of <a href="http://www-users.cs.umn.edu/~kumar/dmbook/index.php">Introduction to Data Mining</a>.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#3">Itemset and Support</a></li><li><a href="#5">Association Rules</a></li><li><a href="#7">Confidence and Lift</a></li><li><a href="#10">Apriori Algorithm</a></li><li><a href="#13">Rule Generation Algorithm</a></li><li><a href="#15">Test Example: Congressional Voting Records</a></li><li><a href="#17">Web Usage Analysis with Clickstream Data</a></li><li><a href="#18">Closing</a></li></ul></div><p>Let's start with loading the example dataset used in the PDF and creating a binary matrix representation of transactions in rows and items in columns, with 1 meaning the given item was in the given transaction, but otherwise 0.</p><pre class="codeinput">clearvars; close <span class="string">all</span>; clc;

transactions = {{<span class="string">'Bread'</span>,<span class="string">'Milk'</span>};<span class="keyword">...</span>
    {<span class="string">'Bread'</span>,<span class="string">'Diapers'</span>,<span class="string">'Beer'</span>,<span class="string">'Eggs'</span>};<span class="keyword">...</span>
    {<span class="string">'Milk'</span>,<span class="string">'Diapers'</span>,<span class="string">'Beer'</span>,<span class="string">'Cola'</span>};<span class="keyword">...</span>
    {<span class="string">'Bread'</span>,<span class="string">'Milk'</span>,<span class="string">'Diapers'</span>,<span class="string">'Beer'</span>};<span class="keyword">...</span>
    {<span class="string">'Bread'</span>,<span class="string">'Milk'</span>,<span class="string">'Diapers'</span>,<span class="string">'Cola'</span>}};

items = unique([transactions{:}]);
T = zeros(size(transactions,1), length(items));
<span class="keyword">for</span> i = 1:size(transactions,1)
    T(i,ismember(items,transactions{i,:})) = 1;
<span class="keyword">end</span>
disp(array2table(T,<span class="string">'VariableNames'</span>,items))
</pre><pre class="codeoutput">    Beer    Bread    Cola    Diapers    Eggs    Milk
    ____    _____    ____    _______    ____    ____

    0       1        0       0          0       1   
    1       1        0       1          1       0   
    1       0        1       1          0       1   
    1       1        0       1          0       1   
    0       1        1       1          0       1   

</pre><p>As the first step to understand association analysis, let's go over some basic concepts, such as itemsets, support, confidence, etc.</p><h2>Itemset and Support<a name="3"></a></h2><p>The columns of the table shows all the items in the datasets. You can see their names. A subset of those items in any combination is an itemset, and if it contains only one item, it is a 1-itemset, if two, it is a 2-itemset... a k-itemset where k is the number of the columns, meaning everything is in that itemset. If it contains no item, it is a null itemset. For example, this is a 3-itemset:</p><pre>{Beer, Diapers, Milk}</pre><p>A transaction can contains multiple itemsets as its subsets. For example, an itemset <tt>{Bread, Diapers}</tt> is contained in the second transaction in the example dataset (see below), but not <tt>{Bread, Milk}</tt>.</p><pre>{Bread, Diapers, Beer, Eggs}</pre><p>Support count is the count of how often a given itemset appears across all the transactions. How frequent a given itemset appear is given by a metric called support.</p><pre>Support = itemset support count / number of transactions</pre><p>Here is an example of how you compute it.</p><pre class="codeinput">itemset = {<span class="string">'Beer'</span>,<span class="string">'Diapers'</span>,<span class="string">'Milk'</span>};
fprintf(<span class="string">'Itemset: {%s, %s, %s}\n'</span>, itemset{:})
cols = ismember(items,itemset); <span class="comment">% get col indices of items in itemset</span>
N = size(T,1);
fprintf(<span class="string">'Number of transactions = %d\n'</span>,N)
supportCount = sum(all(T(:,cols),2)); <span class="comment">% count rows that include all items</span>
fprintf(<span class="string">'Support Count for this itemset = %d\n'</span>,supportCount)
itemSetSupport = supportCount/size(T,1);
fprintf(<span class="string">'Support = %.2f (= support count / number of transactions)\n'</span>,itemSetSupport)
</pre><pre class="codeoutput">Itemset: {Beer, Diapers, Milk}
Number of transactions = 5
Support Count for this itemset = 2
Support = 0.40 (= support count / number of transactions)
</pre><h2>Association Rules<a name="5"></a></h2><p>Association rules are made up of antecedents (ante) and consequents (conseq) and take the following form:</p><pre>{ante} =&gt; {conseq}</pre><p>A k-itemset where k &gt; 1 can be randomly divided into ante and conseq to form such a rule. Here is an example:</p><pre class="codeinput">ante = {<span class="string">'Diapers'</span>,<span class="string">'Milk'</span>};
conseq = setdiff(itemset,ante); <span class="comment">% get items not in |ante|</span>
fprintf(<span class="string">'Itemset: {%s, %s, %s}\n'</span>, itemset{:})
fprintf(<span class="string">'Ante   : {%s, %s}\n'</span>,ante{:})
fprintf(<span class="string">'Conseq : {%s}\n'</span>,conseq{:})
fprintf(<span class="string">'Rule   : {%s, %s} =&gt; {%s}\n'</span>, ante{:},conseq{:})
</pre><pre class="codeoutput">Itemset: {Beer, Diapers, Milk}
Ante   : {Diapers, Milk}
Conseq : {Beer}
Rule   : {Diapers, Milk} =&gt; {Beer}
</pre><p>You can think of this rule as "when diapers and mile appear in the same transaction, you often see beer in the same transaction as well". How strong is this association rule?</p><h2>Confidence and Lift<a name="7"></a></h2><p>The most basic measure of strength is confidence, which tells us how often a given rule applies within the transactions that contain the ante.</p><p>A given rule applies when all items from both antecedents and consequents are present in a transaction, so it is the same thing as an itemset that contains the same items. So we can use the support metric for the itemset to compute confidence.</p><pre>Confidence = itemset support / ante support</pre><p>Here is an example.</p><pre class="codeinput">cols = ismember(items,ante);
anteCount = sum(all(T(:,cols),2));
fprintf(<span class="string">'Support Count for Ante = %d\n'</span>,anteCount)
anteSupport = anteCount/N;
fprintf(<span class="string">'Support for Ante = %.2f\n'</span>,anteSupport)
confidence = itemSetSupport/anteSupport;
fprintf(<span class="string">'Confidence = %.2f (= itemset support / ante support)\n'</span>,confidence)
</pre><pre class="codeoutput">Support Count for Ante = 3
Support for Ante = 0.60
Confidence = 0.67 (= itemset support / ante support)
</pre><p>Another measure of strength is lift. It compares the probability of ante and conseq happening together independently to the observed frequency of such combination. We can use respective support metrics to make this comparison.</p><pre>Lift = itemset support / (ante support x conseq support)</pre><pre class="codeinput">cols = ismember(items,conseq);
conseqCount = sum(all(T(:,cols),2));
fprintf(<span class="string">'Support Count for Conseq = %d\n'</span>,conseqCount)
conseqSupport = conseqCount/N;
fprintf(<span class="string">'Support for Conseq = %.2f\n'</span>,conseqSupport)
lift = itemSetSupport/(anteSupport*conseqSupport);
fprintf(<span class="string">'Lift = %.2f (= itemset support / (ante support x conseq support))\n'</span>,lift)
</pre><pre class="codeoutput">Support Count for Conseq = 3
Support for Conseq = 0.60
Lift = 1.11 (= itemset support / (ante support x conseq support))
</pre><p>If lift is 1, then the probabilities of ante and conseq occurring together is independent and there is no special relationship. If it is larger than 1, then lift tells us how strongly ante and conseq are dependent to to each other.</p><h2>Apriori Algorithm<a name="10"></a></h2><p>Now we know the basic concepts, we can define the goal of our analysis as finding association rules with sufficient level of support (happens often enough) and confidence (association is strong). Lift can be another criteria to measure the strength.</p><div><ol><li>Generate frequent itemset that clear the minimum support threshold recursively from 1-itemsets to higher level itemsets, pruning candidates along the way - see <tt>findFreqItemsets.m</tt></li><li>Generate rules that clear the minimum confidence threshold in a similar way - see <tt>generateRules.m</tt></li></ol></div><p>The brute force method of those steps would have you calculate the support and confidence of all possible itemset combinations, but that would be computationally expensive, because number of candidates grows exponentially.</p><p>Apriori algorithm addresses this issue by generating candidates selectively. To get an intuition, think about the frequency of an itemset that contains some infrequent items. That itemset will never be more frequent than the least frequent item it contains. So if you construct your candidates by combining the frequent itemsets only, starting from 1-itemset and continue to higher levels, then you avoid creating useless candidates. You can see this in action in <tt>aprioriGen.m</tt>.</p><p>Let's start with generating frequent itemsets and get their support measures. Function <tt>findFreqItemsets()</tt> takes cell array of vectors that represents the indices of items in <tt>items</tt> cell array. First we need to convert <tt>transactions</tt>, which is a cell array of strings, into this format. I created <tt>numeralize()</tt> for this purpose.</p><pre class="codeinput">C = numeralize(transactions);

minSup = 0.6; <span class="comment">% minimum support threshold 0.6</span>
[F,S] = findFreqItemsets(C,minSup);
fprintf(<span class="string">'Minimum Support        : %.2f\n'</span>, minSup)
fprintf(<span class="string">'Frequent Itemsets Found: %d\n'</span>, sum(arrayfun(@(x) length(x.freqSets), F)))
fprintf(<span class="string">'Max Level Reached      : %d-itemsets\n'</span>, length(F))
fprintf(<span class="string">'Number of Support Data : %d\n'</span>, length(S))
</pre><pre class="codeoutput">Minimum Support        : 0.60
Frequent Itemsets Found: 8
Max Level Reached      : 2-itemsets
Number of Support Data : 13
</pre><p>When we computed support for each itemset we evaluated, we stored the result in a <a href="http://www.mathworks.com/help/matlab/matlab_prog/overview-of-the-map-data-structure.html">Map object</a> <tt>S</tt>. This is used for rule generation in order to avoid recalculating support as part of confidence computation. You can now retrieve support for a given itemset with this object, by supplying the string representation of the itemset as key. Let's try [2,4,6]:</p><pre class="codeinput">itemset = [2,4,6];
fprintf(<span class="string">'Support for the itemset {%s %s %s}: %.2f\n'</span>,<span class="keyword">...</span>
    items{itemset(:)},S(num2str(itemset))) <span class="comment">% num2str() converts a vector to string</span>
</pre><pre class="codeoutput">Support for the itemset {Bread Diapers Milk}: 0.40
</pre><p>This itemset clearly didn't meet the minimum support criteria.</p><h2>Rule Generation Algorithm<a name="13"></a></h2><p>We saw earlier that you can generate rule candidates from frequent itemsets by splitting their contents into antecedents and consequents, and computing their confidence.</p><p>If we generate every possible candidates by such brute force method, it will be very time consuming. Apriori algorithm is also used to generate rules selectively. Let's say that this rule has low confidence.</p><pre>{Beer, Diapers} =&gt; {Milk}</pre><p>Then any other rules generated from this itemset that contain <tt>{Milk}</tt> in rule consequent will have low confidence.</p><pre>{Beer} =&gt; {Diapers, Milk}
{Diapers} =&gt; {Beer, Milk}</pre><p>Why? because support for those antecedents will be always greater than the initial antecedent <tt>{Beer, Diapers}</tt>, while the support for the itemset (hence also for the rule) remains the same, and confidence is based on the ratio of support between the rule and the antecedent.</p><p>We can take advantage of this intuition by first generating rules with only one item in consequent and drop those that doesn't meet the minimum criteria, and then merge those consequents to generate rules with two items in consequents, and so forth.</p><p>Now we can generate association rules from the frequent itemsets we generated in the previous step.</p><pre class="codeinput">minConf = 0.8; <span class="comment">% minimum confidence threshold 0.8</span>
rules = generateRules(F,S,minConf);
fprintf(<span class="string">'Minimum Confidence     : %.2f\n'</span>, minConf)
fprintf(<span class="string">'Rules Found            : %d\n\n'</span>, length(rules))

<span class="keyword">for</span> i = 1:length(rules)
    disp([sprintf(<span class="string">'{%s}'</span>,items{rules(i).Ante}),<span class="string">' =&gt; '</span>,<span class="keyword">...</span>
        sprintf(<span class="string">'{%s}'</span>, items{rules(i).Conseq}),<span class="keyword">...</span>
        sprintf(<span class="string">'     Conf: %.2f '</span>,rules(i).Conf),<span class="keyword">...</span>
        sprintf(<span class="string">'Lift: %.2f '</span>,rules(i).Lift),<span class="keyword">...</span>
        sprintf(<span class="string">'Sup: %.2f'</span>,rules(i).Sup)])
<span class="keyword">end</span>
</pre><pre class="codeoutput">Minimum Confidence     : 0.80
Rules Found            : 1

{Beer} =&gt; {Diapers}     Conf: 1.00 Lift: 1.25 Sup: 0.60
</pre><p>With minimum support 0.6 and minimum confidence 0.8 we found only one rule that clear those thresholds: <tt>{Beer} =&gt; {Diapers}</tt>. Confidence is 1.00, which means we see diapers in all transactions that includes beer, and lift is 1.25, so they are fairly strongly associated.</p><p>You can also use <a href="http://www.mathworks.com/help/bioinfo/ref/biograph.html">biograph class</a> in Bioinformatics Toolbox to visualize the connections among items as a directed graph. First you need to generate an adjacency matrix of antecedents and consequents from the rules.</p><pre class="codeinput">ante = arrayfun(@(x) x.Ante, rules); <span class="comment">% get the antes as a vector</span>
conseq = arrayfun(@(x) x.Conseq, rules); <span class="comment">% get the conseqs as a vector</span>
<span class="comment">% create an adjacency matrix (it is a sparse matrix)</span>
AdjMat = sparse(ante,conseq,ones(1,length(ante)),length(items),length(items));
<span class="comment">% create a biograph object from the matrix</span>
graph = biograph(AdjMat,items);
<span class="comment">% visualize the graph.</span>
view(graph)
</pre><img vspace="5" hspace="5" src="marketBasket_01.png" alt=""> <h2>Test Example: Congressional Voting Records<a name="15"></a></h2><p>The textbook uses <a href="https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records">1984 Congressional Voting Records</a> dataset from UCI Machine Learning Repository. We can test our code against the result in the textbook by running <tt>congressionalVotes.m</tt>.</p><pre class="codeinput">congressionalVotes
</pre><pre class="codeoutput">Minimum Support        : 0.30
Frequent Itemsets Found: 1026
Max Level Reached      : 7-itemsets
Number of Support Data : 2530
Minimum Confidence     : 0.90
Rules Found            : 2942

{El Salvador = Yes, Budget Resolution = No, Mx Missile = No} =&gt; {Republican}
   Conf: 0.91 Lift: 2.36 Sup: 0.30
   Correct!  Expected Conf 0.91

{Budget Resolution = Yes, Mx Missile = Yes, El Salvador = No} =&gt; {Democrat}
   Conf: 0.97 Lift: 1.59 Sup: 0.36
   Correct!  Expected Conf 0.97

{Physician Fee Freeze = Yes, Right To Sue = Yes, Crime = Yes} =&gt; {Republican}
   Conf: 0.94 Lift: 2.42 Sup: 0.30
   Correct!  Expected Conf 0.94

{Physician Fee Freeze = No, Right To Sue = No, Crime = No} =&gt; {Democrat}
   Conf: 1.00 Lift: 1.63 Sup: 0.31
   Correct!  Expected Conf 1.00

</pre><img vspace="5" hspace="5" src="marketBasket_02.png" alt=""> <img vspace="5" hspace="5" src="marketBasket_03.png" alt=""> <p>You see that rules we generated match with those in the textbook. The plot of support, confidence and lift is useful to identify rules that are high support, high confidence (upper right region of the plot) and high lift (redder). If you type <tt>gname</tt> into MATLAB prompt, you can interactively identify the indices of those points, and hit <tt>Enter</tt> to end the interactive session.</p><p><img vspace="5" hspace="5" src="gname.png" alt=""> </p><pre class="codeinput">disp(<span class="string">'Rule ID = 51'</span>)
fprintf(<span class="string">'{%s, %s} =&gt; {%s}, Conf: %.2f\n'</span>,<span class="keyword">...</span>
    vars{rules(51).Ante(1)},vars{rules(51).Ante(2)},<span class="keyword">...</span>
    vars{rules(51).Conseq},rules(51).Conf)
</pre><pre class="codeoutput">Rule ID = 51
{Budget Resolution = Yes, Physician Fee Freeze = No} =&gt; {Democrat}, Conf: 1.00
</pre><h2>Web Usage Analysis with Clickstream Data<a name="17"></a></h2><p>One of non-supermarket use cases of association analysis is clickstream data analysis. Clickstream records series of web pages a web visitor goes through in a session, usually extracted from web server logs or generated with a tracking code embedded on web pages.</p><p>We will use the 'kosarak' dataset from <a href="http://fimi.ua.ac.be/data/">Frequent Itemset Mining Dataset Repository</a> which contains anonymized clickstream data of a Hungarian on-line news portal. This is a text file with 990,003 rows of data. I will only use the first 10,000 rows to speed up the computation. No metadata is available to understand what those pages are, unfortunately.</p><p>Let's run <tt>kosarak.m</tt> to see association rules from this dataset.</p><pre class="codeinput">kosarak
</pre><pre class="codeoutput">Showing the first five rows of the dataset...
           1           2           3

           1

           4           5           6           7

           1           8

           9          10

Processing dataset with minimum support threshold = 0.03
...
Frequent Itemsets Found: 63
Max Level              : k = 4
Number of Support Data : 10294

Minimum Confidence     : 0.80
Rules Found            : 34

</pre><img vspace="5" hspace="5" src="marketBasket_04.png" alt=""> <h2>Closing<a name="18"></a></h2><p>We started with a simple supermarket example, moved on to congressioal votes records, and finally web usage analysis, but we also see that you can't go far without metadata. In the next step, I would like to apply this to a real life web analytics data and see if we can learn something interesting.</p><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2014a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Market Basket Analysis with MATLAB% When you hear "Data Mining", the "beer and diapers" story comes to mind% - supermarkets placing beer next to diapers because they mined their POS% (Point-of-Sales) data and found that men often bought those two items % together. % % This is known as *Association Analysis* or *Frequent Itemset Mining*. I % don't work at a supermarket but I wanted to play with this technique for % web usage pattern mining. Fortunately, a pseudo code for this algorithm % is available in Chapter 6 (sample chapter available for free download) % of <http://www-users.cs.umn.edu/~kumar/dmbook/index.php Introduction to % Data Mining>.%%%% Let's start with loading the example dataset used in the PDF and % creating a binary matrix representation of transactions in rows and items % in columns, with 1 meaning the given item was in the given transaction, % but otherwise 0. clearvars; close all; clc;transactions = {{'Bread','Milk'};...     {'Bread','Diapers','Beer','Eggs'};...    {'Milk','Diapers','Beer','Cola'};...    {'Bread','Milk','Diapers','Beer'};...    {'Bread','Milk','Diapers','Cola'}};items = unique([transactions{:}]);T = zeros(size(transactions,1), length(items));for i = 1:size(transactions,1)    T(i,ismember(items,transactions{i,:})) = 1;enddisp(array2table(T,'VariableNames',items))%%% As the first step to understand association analysis, let's go over some % basic concepts, such as itemsets, support, confidence, etc.% %% Itemset and Support% The columns of the table shows all the items in the datasets. You can see% their names. A subset of those items in any combination is an itemset,% and if it contains only one item, it is a 1-itemset, if two, it is a% 2-itemset... a k-itemset where k is the number of the columns, meaning% everything is in that itemset. If it contains no item, it is a null  % itemset. For example, this is a 3-itemset:%%  {Beer, Diapers, Milk}%% A transaction can contains multiple itemsets as its subsets. For% example, an itemset |{Bread, Diapers}| is contained in the second% transaction in the example dataset (see below), but not |{Bread, Milk}|. %%  {Bread, Diapers, Beer, Eggs}%%%% Support count is the count of how often a given itemset appears across % all the transactions. How frequent a given itemset appear is given by a% metric called support. % %  Support = itemset support count / number of transactions%% Here is an example of how you compute it. itemset = {'Beer','Diapers','Milk'};fprintf('Itemset: {%s, %s, %s}\n', itemset{:})cols = ismember(items,itemset); % get col indices of items in itemsetN = size(T,1);fprintf('Number of transactions = %d\n',N)supportCount = sum(all(T(:,cols),2)); % count rows that include all itemsfprintf('Support Count for this itemset = %d\n',supportCount)itemSetSupport = supportCount/size(T,1);fprintf('Support = %.2f (= support count / number of transactions)\n',itemSetSupport)%% Association Rules% Association rules are made up of antecedents (ante) and consequents % (conseq) and take the following form:%%  {ante} => {conseq} %% A k-itemset where k > 1 can be randomly divided into ante and conseq to % form such a rule. Here is an example:ante = {'Diapers','Milk'};conseq = setdiff(itemset,ante); % get items not in |ante|fprintf('Itemset: {%s, %s, %s}\n', itemset{:})fprintf('Ante   : {%s, %s}\n',ante{:})fprintf('Conseq : {%s}\n',conseq{:})fprintf('Rule   : {%s, %s} => {%s}\n', ante{:},conseq{:})%%% You can think of this rule as "when diapers and mile appear in the same% transaction, you often see beer in the same transaction as well". How % strong is this association rule? %% Confidence and Lift% The most basic measure of strength is confidence, which tells us how% often a given rule applies within the transactions that contain the ante.% % A given rule applies when all items from both antecedents and consequents% are present in a transaction, so it is the same thing as an itemset that% contains the same items. So we can use the support metric for the itemset% to compute confidence.% %  Confidence = itemset support / ante support%% Here is an example. cols = ismember(items,ante);anteCount = sum(all(T(:,cols),2));fprintf('Support Count for Ante = %d\n',anteCount)anteSupport = anteCount/N;fprintf('Support for Ante = %.2f\n',anteSupport)confidence = itemSetSupport/anteSupport;fprintf('Confidence = %.2f (= itemset support / ante support)\n',confidence)%%% Another measure of strength is lift. It compares the probability of ante% and conseq happening together independently to the observed frequency of% such combination. We can use respective support metrics to make this % comparison.  %%  Lift = itemset support / (ante support x conseq support)cols = ismember(items,conseq);conseqCount = sum(all(T(:,cols),2));fprintf('Support Count for Conseq = %d\n',conseqCount)conseqSupport = conseqCount/N;fprintf('Support for Conseq = %.2f\n',conseqSupport)lift = itemSetSupport/(anteSupport*conseqSupport);fprintf('Lift = %.2f (= itemset support / (ante support x conseq support))\n',lift)%% % If lift is 1, then the probabilities of ante and conseq occurring% together is independent and there is no special relationship. If it is% larger than 1, then lift tells us how strongly ante and conseq are% dependent to to each other.%% Apriori Algorithm% Now we know the basic concepts, we can define the goal of our analysis as% finding association rules with sufficient level of support (happens % often enough) and confidence (association is strong). Lift can be another % criteria to measure the strength. % % # Generate frequent itemset that clear the minimum support threshold% recursively from 1-itemsets to higher level itemsets, pruning candidates% along the way - see |findFreqItemsets.m|% # Generate rules that clear the minimum confidence threshold in a similar% way - see |generateRules.m|% % The brute force method of those steps would have you calculate the% support and confidence of all possible itemset combinations, but that% would be computationally expensive, because number of candidates grows% exponentially.%% Apriori algorithm addresses this issue by generating candidates % selectively. To get an intuition, think about the frequency of an itemset % that contains some infrequent items. That itemset will never be more% frequent than the least frequent item it contains. So if you construct % your candidates by combining the frequent itemsets only, starting from% 1-itemset and continue to higher levels, then you avoid creating useless % candidates. You can see this in action in |aprioriGen.m|. %% Let's start with generating frequent itemsets and get their support% measures. Function |findFreqItemsets()| takes cell array of vectors that% represents the indices of items in |items| cell array. First we need to% convert |transactions|, which is a cell array of strings, into this % format. I created |numeralize()| for this purpose.C = numeralize(transactions);minSup = 0.6; % minimum support threshold 0.6[F,S] = findFreqItemsets(C,minSup);fprintf('Minimum Support        : %.2f\n', minSup)fprintf('Frequent Itemsets Found: %d\n', sum(arrayfun(@(x) length(x.freqSets), F)))fprintf('Max Level Reached      : %d-itemsets\n', length(F))fprintf('Number of Support Data : %d\n', length(S))%%% When we computed support for each itemset we evaluated, we stored the% result in a % <http://www.mathworks.com/help/matlab/matlab_prog/overview-of-the-map-data-structure.html % Map object> |S|. This is used for rule generation in order to avoid% recalculating support as part of confidence computation. You can now % retrieve support for a given itemset with this object, by supplying the % string representation of the itemset as key. Let's try [2,4,6]:itemset = [2,4,6];fprintf('Support for the itemset {%s %s %s}: %.2f\n',...    items{itemset(:)},S(num2str(itemset))) % num2str() converts a vector to string%% % This itemset clearly didn't meet the minimum support criteria. %%% Rule Generation Algorithm% We saw earlier that you can generate rule candidates from frequent % itemsets by splitting their contents into antecedents and consequents, % and computing their confidence. % % If we generate every possible candidates by such brute force method, it % will be very time consuming. Apriori algorithm is also used to generate % rules selectively. Let's say that this rule has low confidence. % %  {Beer, Diapers} => {Milk}% % Then any other rules generated from this itemset that contain |{Milk}| % in rule consequent will have low confidence. % %  {Beer} => {Diapers, Milk}%  {Diapers} => {Beer, Milk}%% Why? because support for those antecedents will be always greater than % the initial antecedent |{Beer, Diapers}|, while the support for the% itemset (hence also for the rule) remains the same, and confidence is% based on the ratio of support between the rule and the antecedent. % % We can take advantage of this intuition by first generating rules with % only one item in consequent and drop those that doesn't meet the minimum % criteria, and then merge those consequents to generate rules with two % items in consequents, and so forth. %% Now we can generate association rules from the frequent itemsets we% generated in the previous step. minConf = 0.8; % minimum confidence threshold 0.8rules = generateRules(F,S,minConf);fprintf('Minimum Confidence     : %.2f\n', minConf)fprintf('Rules Found            : %d\n\n', length(rules))for i = 1:length(rules)    disp([sprintf('{%s}',items{rules(i).Ante}),' => ',...        sprintf('{%s}', items{rules(i).Conseq}),...        sprintf('     Conf: %.2f ',rules(i).Conf),...        sprintf('Lift: %.2f ',rules(i).Lift),...        sprintf('Sup: %.2f',rules(i).Sup)])end%%% With minimum support 0.6 and minimum confidence 0.8 we found only one% rule that clear those thresholds: |{Beer} => {Diapers}|. Confidence is % 1.00, which means we see diapers in all transactions that includes beer, % and lift is 1.25, so they are fairly strongly associated.% % You can also use <http://www.mathworks.com/help/bioinfo/ref/biograph.html % biograph class> in Bioinformatics Toolbox to visualize the connections % among items as a directed graph. First you need to generate an adjacency % matrix of antecedents and consequents from the rules. ante = arrayfun(@(x) x.Ante, rules); % get the antes as a vectorconseq = arrayfun(@(x) x.Conseq, rules); % get the conseqs as a vector% create an adjacency matrix (it is a sparse matrix)AdjMat = sparse(ante,conseq,ones(1,length(ante)),length(items),length(items));% create a biograph object from the matrixgraph = biograph(AdjMat,items);% visualize the graph. view(graph)%% Test Example: Congressional Voting Records% The textbook uses <https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records % 1984 Congressional Voting Records> dataset from UCI Machine Learning % Repository. We can test our code against the result in the textbook by% running |congressionalVotes.m|.congressionalVotes%%% You see that rules we generated match with those in the textbook. The% plot of support, confidence and lift is useful to identify rules that are% high support, high confidence (upper right region of the plot) and high% lift (redder). If you type |gname| into MATLAB prompt, you can% interactively identify the indices of those points, and hit |Enter| to% end the interactive session.%% <<gname.png>>%disp('Rule ID = 51')fprintf('{%s, %s} => {%s}, Conf: %.2f\n',...    vars{rules(51).Ante(1)},vars{rules(51).Ante(2)},...    vars{rules(51).Conseq},rules(51).Conf)%% Web Usage Analysis with Clickstream Data% One of non-supermarket use cases of association analysis is clickstream % data analysis. Clickstream records series of web pages a web visitor% goes through in a session, usually extracted from web server logs or% generated with a tracking code embedded on web pages. % % We will use the 'kosarak' dataset from % <http://fimi.ua.ac.be/data/ Frequent Itemset Mining Dataset Repository>% which contains anonymized clickstream data of a Hungarian on-line % news portal. This is a text file with 990,003 rows of data. I will only% use the first 10,000 rows to speed up the computation. No metadata is % available to understand what those pages are, unfortunately. %% Let's run |kosarak.m| to see association rules from this dataset.kosarak%% Closing% We started with a simple supermarket example, moved on to congressioal % votes records, and finally web usage analysis, but we also see that you % can't go far without metadata. In the next step, I would like to apply % this to a real life web analytics data and see if we can learn something% interesting.
##### SOURCE END #####
--></body></html>